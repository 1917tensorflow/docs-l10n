{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ic4_occAAiAT"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "yCl0eTNH5RS3"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# 영화 리뷰를 사용한 텍스트 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKY4XMc9o8iB"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />TensorFlow.org에서 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩(Colab)에서 실행하기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />깃허브(GitHub) 소스 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />노트북에 다운로드</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "본 튜토리얼은 영화 리뷰(review) 텍스트를 긍정(positive) 또는 부정(negative)으로 분류하는 방법에 대해 소개합니다. 이 예제는 클래스(class)가 두 개인 이진 분류(binary classification) 문제의 일종입니다. 이진 분류는 중요하고 흔히 볼 수 있는 머신러닝 문제들 중 하나입니다.\n",
        "\n",
        "여기에서는 [인터넷 영화 데이터베이스](https://www.imdb.com/)(Internet Movie Database)에서 수집한 50,000개의 영화 리뷰 텍스트를 담은 [IMDB 데이터셋](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)을 사용하겠습니다. 25,000개 리뷰는 훈련용으로, 25,000개는 테스트용으로 나뉘어져 있습니다. 훈련 세트와 테스트 세트의 클래스는 *균형*이 잡혀 있습니다. 즉 긍정적인 리뷰와 부정적인 리뷰의 개수가 동일합니다.\n",
        "\n",
        "이 노트북은 모델을 만들고 훈련하기 위해 텐서플로의 고수준 파이썬 API인 [tf.keras](https://www.tensorflow.org/guide/keras)를 사용합니다. `tf.keras`를 사용한 고급 텍스트 분류 튜토리얼은 [MLCC 텍스트 분류 가이드](https://developers.google.com/machine-learning/guides/text-classification/)를 참고하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8vdyFn79gt1L"
      },
      "source": [
        "## 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2ew7HTbPpCJH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iAsKG535pHep"
      },
      "source": [
        "<a id=\"download\"></a>\n",
        "\n",
        "## IMDB 데이터셋 다운로드\n",
        "\n",
        "IMDB 영화 리뷰 데이터셋은 `tfds`으로 패키지 되어있습니다. 그것은 이미 사전 처리되어 각 정수가 사전의 특정 단어를 나타내는 정수의 순서에 따라 검토(단어의 순서)가 변환되었습니다.\n",
        "\n",
        "IMDB 데이터 집합을 시스템에 다운로드하는 코드 (이미 다운로드한 경우 캐시된 복사본 사용):\n",
        "\n",
        "텍스트를 인코딩하려면 [튜토리얼 문서](../load_data/text.ipynb)를 참조하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wbIQ2wSeXSme"
      },
      "outputs": [],
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    # 사전 인코딩된 버전(단어 8k) 사용.\n",
        "    'imdb_reviews/subwords8k', \n",
        "    # 교육/테스트 데이터셋을 튜플로 반환.\n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    # 사전 대신 데이터 집합에서 쌍(예: 레이블)을 반환.\n",
        "    as_supervised=True,\n",
        "    # info 구조도 반납. \n",
        "    with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qvA8HYDJj8OU"
      },
      "source": [
        "<a id=\"encoder\"></a>\n",
        "\n",
        "## 인코더 시도\n",
        "\n",
        " 데이터셋 `info` 는 텍스트 인코더를 포함한다. (`tfds.features.text.SubwordTextEncoder`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EplYp5pNnW1S"
      },
      "outputs": [],
      "source": [
        "encoder = info.features['text'].encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e7ACuHM5hFp3"
      },
      "outputs": [],
      "source": [
        "print ('단어 사이즈: {}'.format(encoder.vocab_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAfGg8YRe6fu"
      },
      "source": [
        "이 텍스트 인코더는 모든 문자열을 역방향으로 인코딩합니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Bq6xDmf2SAs-"
      },
      "outputs": [],
      "source": [
        "sample_string = 'Hello TensorFlow.'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print ('인코딩된 문자열: {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print ('원래의 문자열: \"{}\"'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TbhM970AVA8w"
      },
      "source": [
        "인코더는 그 단어가 사전에 없을 경우 그것을 하위 단어나 문자로 분해하여 문자열을 인코딩합니다. 따라서 문자열이 데이터 집합을 더 많이 닮을수록 인코딩된 표현은 더 짧아질 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GUIRWSO8yxT5"
      },
      "outputs": [],
      "source": [
        "for ts in encoded_string:\n",
        "  print ('{} ----> {}'.format(ts, encoder.decode([ts])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "## 데이터 탐색\n",
        "\n",
        "데이터의 형식을 이해하는 시간을 갖습니다. 데이터 집합은 사전 처리됩니다. 각 예는 영화 리뷰의 단어를 나타내는 정수들의 배열입니다. \n",
        "\n",
        "리뷰 텍스트는 정수로 전환되었는데, 각 정수는 사전의 특정 단어 부분을 나타내고 있습니다. \n",
        "\n",
        "각 레이블은 0 또는 1의 정수 값이며, 여기서 0은 음의 리뷰, 1은 양의 리뷰입니다.\n",
        "\n",
        "첫 번째 리뷰는 다음과 같습니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cxnWQJijdGA1"
      },
      "outputs": [],
      "source": [
        "for train_example, train_label in train_data.take(1):\n",
        "  print('인코딩된 문자:', train_example[:10].numpy())\n",
        "  print('레이블:', train_label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wy0v9Hs4v41q"
      },
      "source": [
        "`info` 구조는 인코더/디코더를 포함하고 있습니다. 인코더를 사용하여 원본 텍스트를 복구할 수 있습니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "34VUXtgxsVpf"
      },
      "outputs": [],
      "source": [
        "encoder.decode(train_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qJmTiO0IYAjm"
      },
      "source": [
        "## 교육을 위한 데이터 준비\n",
        "\n",
        "모형에 대한 교육 데이터 묶음을 생성하려고 할 것입니다. 리뷰의 길이가 모두 다르므로 batching하는 동안 padded_batch를 사용하여 시퀀스를 0으로 패딩하세요.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SDRI_s_tX1Hk"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = (\n",
        "    train_data\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .padded_batch(32))\n",
        "\n",
        "test_batches = (\n",
        "    test_data\n",
        "    .padded_batch(32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9D9pIr0JwvRl"
      },
      "source": [
        "패딩은 동적이기 때문에 각 배치의 길이가 서로 다르기 때문에 각 배치의 모양은 (batch_size, sequence_length)입니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sXXne4DreQfv"
      },
      "outputs": [],
      "source": [
        "for example_batch, label_batch in train_batches.take(2):\n",
        "  print(\"배치 모양:\", example_batch.shape)\n",
        "  print(\"레이블 모양:\", label_batch.shape)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLC02j2g-llC"
      },
      "source": [
        "## 모델 빌드\n",
        "\n",
        "신경망은 층을 쌓아서 생성됩니다. 이것은 두 가지 주요한 구조적 결정이 필요합니다.:\n",
        "\n",
        "* 모델에서 사용할 레이어 수는?\n",
        "* 각 레이어에 사용할 *숨겨진 유닛*은 몇 개인가?\n",
        "\n",
        "이 예에서 입력 데이터는 단어 지수의 배열로 구성됩니다. 예측할 레이블은 0 또는 1입니다. 이 문제에 대한 \"연속적인 단어 백\" 스타일 모델을 구축해 봅시다.:\n",
        "\n",
        "경고: 이 모델은 마스킹을 사용하지 않기 때문에 제로 패딩이 입력의 일부로 사용되므로 패딩 길이가 출력에 영향을 줄 수 있습니다.  이것을 고치기 위해서, [마스킹 및 패딩 가이드](../../guide/keras/masking_and_padding.ipynb)을 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xpKOoWgu-llD"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(encoder.vocab_size, 16),\n",
        "  keras.layers.GlobalAveragePooling1D(),\n",
        "  keras.layers.Dense(1)])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "레이어들은 클래시파이어를 만들기위해 순차적으로 쌓입니다.:\n",
        "\n",
        "1. 첫번째 레이어는 '임베딩' 레이어 입니다. 이 레이어는 정수로 부호화된 단어를 가져와서 각 단어 지표에 대한 임베딩 벡터를 찾습니다. 이러한 벡터들은 모델 기차들로 실행됩니다. 벡터는 출력 배열에 차원을 더해줍니다. 결과의 차원은 `(배치, 순서, 삽입)`입니다.  *장치에 대해서 더 알고 싶다면, [단어 장치 튜토리얼](../text/word_embeddings.ipynb)에서 확인 할 수 있습니다..*\n",
        "2. 다음, `GlobalAveragePooling1D` 레이어는 순서 차원을 평균을 내어서 각 예제에 대한 고정 길이 출력 벡터를 반환합니다. 이를 통해 모델은 가변 길이의 입력을 가장 간단한 방식으로 처리 할 수 있습니다.\n",
        "3. 고정 길이 출력 벡터는 16개의 숨겨진 단위로 완전히 연결된 레이어를 통해 이동됩니다.\n",
        "4. 이 레이어는 수치 안정성을 위해 로짓을 출력하는 디폴트 선형의 활성화 함수를 사용합니다. 다른 옵션은 확률 또는 참 거짓을 나타내는 0에서 1 사이의 부동 소수점 값을 반환하는 시그 모이 드 활성화 함수를 사용하는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### 히든 유닛\n",
        "\n",
        "위의 모델은 입력과 출력 사이에 두 개의 중간 또는 숨겨진 레이어가 있습니다. 출력 수(유닛, 노드 또는 뉴런)는 층을 나타내는 공간의 차원입니다. 즉, 내부표현을 학습할 때 네트워크가 허용하는 자유의 양입니다.\n",
        "\n",
        "모델에 더 많은 숨겨진 단위 (고차원 표현 공간) 및 / 또는 더 많은 레이어가있는 경우 네트워크는 더 복잡한 표현을 실행 할 수 있습니다. 그러나 그것은 네트워크가 계산적으로 비싸지고, 테스트 데이터가 아닌 트레이닝 데이터의 성능을 향상시키는 원치 않는 패턴의 실행을 초래 할 수 있습니다. 이것을 *오버 피팅*이라고하며, 나중에 살펴 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### 손실 함수 및 최적화 도구\n",
        "\n",
        "모델에는 트레이닝을 위해 손실 함수와 최적화가 필요합니다. 이것은 2진법의 분류 문제이고 모델은 로짓 (선형 활성화가있는 단일 단위 레이어)을 출력하므로 binary_crossentropy loss 함수를 사용합니다.\n",
        "\n",
        "이것은 손실 함수를위한 유일한 방법은 아닙니다.  예를 들어 'mean_squared_error'를 선택할 수 있습니다. 그러나 일반적으로 'binary_crossentropy'는 확률을 처리하는 데있어서 더 수월합니다. 그것은 확률 분포 사이의 거리 또는 \"사실 기반의 분포와 예측 사이의거리\"를 측정합니다.\n",
        "\n",
        "나중에 회귀 문제를 조사 할 때 (예 : 주택 가격 예측) 평균 제곱 오차라는 다른 손실 함수를 사용하는 방법을 살펴볼 것입니다.\n",
        "\n",
        "이제 최적화 및 손실 함수를 사용하도록 모델을 구성합시다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "35jv_fzP-llU"
      },
      "source": [
        "## 모델 훈련\n",
        "\n",
        "'Dataset' 개체를 모델의 적합 함수에 전달하여 모델을 교육하세요. 에폭스 수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tXSGrjWZ-llW"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_batches,\n",
        "                    epochs=10,\n",
        "                    validation_data=test_batches,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "## 모델 평가\n",
        "\n",
        "모델이 어떻게 작동하는지 봅시다. 손실(오차를 나타내는 숫자, 낮은 값이 더 낫다) 및 정확도, 이 두개의 값이 반환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_batches)\n",
        "\n",
        "print(\"손실: \", loss)\n",
        "print(\"정확도: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "이 상당히 순진한 접근법은 약 87%의 정확도를 달성합니다. 더 진보된 접근방식으로, 모델은 95%에 가까워져야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "## 시간 경과에 따른 정확도 및 손실 그래프 생성\n",
        "\n",
        "`model.fit()` 은 훈련 중 발생한 모든 사전을 수록한 `History` 객체를 반환합니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VcvSXvhp-llb"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRKsqL40-lle"
      },
      "source": [
        "네 개의 항목이 있습니다. 훈련과 검증 단계에서 모니터링하는 지표들입니다. 훈련 손실과 검증 손실을 그래프로 그려 보고, 훈련 정확도와 검증 정확도도 그래프로 그려서 비교해 보겠습니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nGoYf2Js-lle"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"파란색 점\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"파란 실선\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6hXx-xOv-llh"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # 그림을 초기화합니다.\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "이 그래프에서 점선은 훈련 손실과 훈련 정확도를 나타냅니다. 실선은 검증 손실과 검증 정확도입니다.\n",
        "\n",
        "훈련 손실은 에포크마다 *감소*하고 훈련 정확도는 *증가*한다는 것을 주목하세요. 경사 하강법 최적화를 사용할 때 볼 수 있는 현상입니다. 매 반복마다 최적화 대상의 값을 최소화합니다.\n",
        "\n",
        "하지만 검증 손실과 검증 정확도에서는 그렇지 못합니다. 약 20번째 에포크 이후가 최적점인 것 같습니다. 이는 과대적합 때문입니다. 이전에 본 적 없는 데이터보다 훈련 데이터에서 더 잘 동작합니다. 이 지점부터는 모델이 과도하게 최적화되어 테스트 데이터에서 *일반화*되기 어려운 훈련 데이터의 특정 표현을 학습합니다.\n",
        "\n",
        "여기에서는 과대적합을 막기 위해 단순히 20번째 에포크 근처에서 훈련을 멈출 수 있습니다. 나중에 콜백(callback)을 사용하여 자동으로 이렇게 하는 방법을 배워 보겠습니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
